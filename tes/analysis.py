"""
Calibration and modeling
"""
import matplotlib.pyplot as plt
import numpy as np
from numpy import (
    bitwise_and as and_b, logical_and as and_l, logical_or as or_l,
    logical_not as not_l, bitwise_or as or_b
)
from scipy import signal
from scipy import stats
from scipy.optimize import brentq
from numba import jit
from collections import namedtuple


@jit
def maxima(f, thresh=10):
    """
    Find local maxima of f using rising zero crossings of the gradient of f.

    :param f: function to find maxima for
    :param thresh: Only return a maxima if f[maxima]>thresh

    :return: array of maxima.
    :Notes: Used to find the maxima of the smoothed measurement histogram.
    """
    g = np.gradient(f)
    pos = g > 0
    xings = (pos[:-1] & ~pos[1:]).nonzero()[0] + 1  # indices of the maxima
    return xings[np.where(f[xings] > thresh)]


Guess = namedtuple(
    'Guess', 'hist f bin_c max_i thresholds'
)


def guess_thresholds(data, bins=16000, win_length=200, max_thresh=10):
    """
    Guess the photon number thresholds using a smoothed histogram.

    :param Union[ndarray,iterable,int,float] data: the measurement data.
    :param bins: argument to numpy.histogram.
    :param int win_length: Hanning window length used for smoothing.
    :param int max_thresh: passed to maxima function.

    :return: (hist, f, bin_c, max_i, thresholds) as a named
            tuple. Where hist is the histogram
            generated by np.histogram(), f is the smoothed histogram, bin_c is a
            ndarray of bin centers, max_i are the indices of the maxima in f,
            and thresholds is a ndarray of theshold guesses.

    :notes: The smoothing is achieved by convolving the Hanning window with hist
            which is generated using numpy.histogram. This guess still needs
            some human sanity checks, especially multiple maxima per photon
            peak. Use plot_guess() and adjust window and threshold to remove
            them.
    """
    hist, edges = np.histogram(data, bins=bins)
    bin_w = edges[1] - edges[0]
    bin_c = edges[:-1] + bin_w / 2

    # smooth the histogram
    win = signal.hann(win_length)
    f = signal.convolve(hist, win, mode='same') / sum(win)

    max_i = maxima(f)  # indices of the maxima
    m = [0.0] + list(bin_c[max_i])
    t = [m[i - 1] + (m[i] - m[i - 1]) / 2 for i in range(2, len(m))]
    thresholds = np.array([0.0] + t + [m[-1] + (m[-1] - t[-1])])

    return Guess(hist, f, bin_c, max_i, thresholds)


def plot_guess(hist, f, x, max_i, init_t, figsize=None):
    if figsize is not None:
        fig = plt.figure(figsize=figsize)
    else:
        fig = plt.figure()
    ax = fig.add_axes([0.12, 0.12, 0.87, 0.87])
    ax.plot(
        x[hist.nonzero()[0]], hist[hist.nonzero()[0]], 's',
        markersize=3, markerfacecolor='none', mew=0.5,
        label='histogram bin', color='gray'
    )
    ax.plot(x, f, 'k', lw=1, label='smoothed histogram')
    ax.plot(
        x[max_i], f[max_i], 'o', label='maxima', markerfacecolor='none', mew=1,
        markersize=5, color='r'
    )
    for t in init_t:
        plt.plot([t, t], [0, max(hist)], ':k', lw=0.5)

    ax.set_xlabel('Energy (abitrary units)')
    ax.set_ylabel('count')
    # ax.set_title('Histogram and initial threshold guess')
    plt.legend()
    return fig


# @jit
def refactor_time(events, tdat, fidx, tidx):
    # TODO generalise
    # remove incorporate tick relative times
    #     events = edat.view(event_dt)
    event_time = events['time']
    event_i = 0
    event_time[0] = 0xFFFF
    # tidx[0] is the first tick period ending with tick 1
    for i in range(len(tidx) - 1):
        # i+1 is the tick number
        changed = fidx[
                  tidx[i]['start'] + 1:tidx[i]['stop'] + 1]['changed'
        ].nonzero()[0]
        if len(changed):
            raise NotImplementedError(
                'tick:{} is not homogeneous'.format(i)
            )
        event_count = int(
            (
                    fidx[tidx[i]['stop']]['payload'] +
                    fidx[tidx[i]['stop']]['length'] -
                    fidx[tidx[i]['start']]['payload']
            ) / (fidx[tidx[i]['start']]['event_size'] * 8)
        )
        #         print(event_count)
        event_i += event_count
        time = int(tdat[i + 1]['time']) + int(event_time[event_i])
        #     print(event_i)
        if time > 0xFFFF:
            time = 0xFFFF
        event_time[event_i] = time
    return event_time


def normalised_pdf(x, params, dist=stats.gamma):
    """
    Convenience function, calculate normalised pdf at each x for dist.

    :param union[numpy.ndarray, int] x: calculate the pdf a each x
    :param iterable params: fitted distribution parameters
    :param dist: type of distribution (see scipy.stats)
    :return: ndarray containing pdf(x)
    """
    return dist.pdf(x, *params[:-1])


def scaled_pdf(x, params, dist=stats.gamma):
    """
    Convenience function, calculate pdf at each x for dist, normalisation is
    given by params[-1].

    :param union[numpy.ndarray, int] x: calculate the pdf a each x
    :param iterable params: fitted distribution parameters
    :param dist: type of distribution (see scipy.stats)
    :return: ndarray containing pdf(x)
    """
    return params[-1]*dist.pdf(x, *params[:-1])


def maximisation_step(param_list, normalise=False, dist=stats.gamma):
    """
    Calculate the thresholds for a mixture model that define the data
    partitions.

    :param list param_list: list of parameters for the distributions in the
           mixture model, returned by expectation_maximisation().
    :param bool normalise: calculate thresholds based on a mixture of
           normalised pdf's.
    :param dist: type of distribution
    :return: ndarray of threshold values.
    """
    t = [0]
    for m in range(1, len(param_list)):
        left = param_list[m - 1]
        right = param_list[m]

        if normalise:
            pdf = normalised_pdf
        else:
            pdf = scaled_pdf

        def f(x):
            return pdf(x, left, dist) - pdf(x, right, dist)

        t.append(brentq(f, dist.median(*left[:-1]), dist.median(*right[:-1])))
    return np.array(t)


class MixtureModel(
    namedtuple(
        'MixtureModel', 'param_list thresholds zero_loc log_likelihood '
        'converged dist'
     )
):
    """
    Subclass of namedtuple used to represent a mixture model.

    fields:
        :param_list: list of parameters for the distributions in the mixture.
        :thresholds: the intersection of neighbouring distributions in the
                     mixture.
        :zero_loc: Fixed location parameter used to fit the first distribution,
                   or None if the location parameter was fitted.
        :log_likelihood: the log of the likelihood that the model could produce
                         the data used to construct it.
        :converged: boolean indicating that the expectation maximisation
                    algorithm terminated normally when fitting the model.
        :dist: the type of distribution forming the components of the mixture,
               (see scipy.stats).

    """
    __slots__ = ()

    def save(self, filename):
        """
        save model as .npz file.

        :param filename: filename to save as, excluding extension"
        :return: None
        """
        np.savez(
            filename,
            param_list=self.param_list,
            thresholds=self.thresholds,
            zero_loc=self.zero_loc,
            log_likelihood=self.log_likelihood,
            converged=self.converged,
            dist=self.dist.name
        )

    @staticmethod
    def load(filename):
        """
        load model from .npz file.

        :param filename: filename excluding extension"
        :return: instance of MixtureModel.
        """
        d = np.load(filename+'.npz')
        return MixtureModel(
            d['param_list'],
            d['thresholds'],
            d['zero_loc'],
            d['log_likelihood'],
            d['converged'],
            getattr(stats, str(d['dist']))
        )


@jit
def expectation_maximisation(
    data, thresh, zero_loc=None, dist=stats.gamma, tol=1, max_iter=30,
    verbose=True
):
    """
    Fit a mixture of distributions to data.

    :param ndarray data: the data to fit.
    :param thresh: initial thresholds that divide data into individual
           distributions in the mixture.
    :param Union[int, None] zero_loc: if not None fixes the location parameter
           of the first distribution in the mixture.
    :param dist: the type of distribution to use (see scipy.stats)
    :param float tol: termination tolerance for change in log_likelihood.
    :param int max_iter: max iterations of expectation maximisation to use.
    :param bool verbose: print progress during optimisation.

    :return: (param_list, zero_loc, log_likelihood, converged) as a named tuple.
            Where param_list is a list of parameter values for distribution in
            the mixture, zero_loc is the arg used to fit, log_likelihood is the
            log_likelihood of the fit, converged is a bool indicating normal
            termination.

    :note: Uses The Expectation maximisation algorithm with hard assignment of
           responsibilities.

    """

    i = 1
    if verbose:
        print(
            'Starting expectation maximisation, {} {} distributions'
            .format(len(thresh), dist.name)
        )
    if verbose:
        print('Expectation step:{}'.format(i))
    param_list = expectation_step(data, thresh, zero_loc=zero_loc, dist=dist)
    if verbose:
        print('Maximisation step:{}'.format(i))
    new_thresh = np.array(
        maximisation_step(param_list, dist=dist)
    )
    ll = mixture_model_ll(data, param_list, dist=dist)
    if verbose:
        print('Threshold changes:{!r}'.format(thresh-new_thresh))
        print('log likelihood:{}'.format(ll))

    converged = False
    thresh = new_thresh
    while i < max_iter:
        i += 1
        if verbose:
            print('Expectation step:{}'.format(i))
        new_param_list = expectation_step(
            data, thresh, zero_loc=zero_loc, dist=dist
        )
        if verbose:
            print('Maximisation step:{}'.format(i))
        new_thresh = np.array(
            maximisation_step(new_param_list, dist=dist)
        )
        new_ll = mixture_model_ll(data, new_param_list, dist=dist)
        if verbose:
            print('Threshold changes:{!r}'.format(thresh-new_thresh))
            print('log likelihood:{} change:{}'.format(new_ll, ll-new_ll))

        # terminate if tol met or ll goes down. FIXME is this appropriate?
        if abs(new_ll - ll) <= tol or new_ll < ll:
            if new_ll > ll:
                ll = new_ll
                param_list = new_param_list
            converged = True
            break

        ll = new_ll
        param_list = new_param_list
        thresh = new_thresh

    if verbose:
        if converged:
            print('Converged')
        else:
            print('Maximum iterations reached')
    return MixtureModel(
        param_list,
        maximisation_step(param_list, normalise=True, dist=dist),
        zero_loc, ll, converged, dist
    )


def mixture_model_ll(data, param_list, dist=stats.gamma):
    """
    Calculate the log likelihood of a mixture model.

    :param ndarray data: the data used to construct the model.
    :param param_list: list of parameters for each distribution in the model.
    :param dist: the type of distribution to use (see scipy.stats).
    :return: the log likelihood.
    """

    ll = np.empty((len(param_list), len(data)), dtype=np.float64)
    for i in range(len(param_list)):
        p = param_list[i][:-1]
        ll[i, :] = dist.pdf(data, *p) * param_list[i][-1]
    return np.sum(np.log(np.sum(ll, 0)))


def expectation_step(
        data, thresholds, zero_loc=None, dist=stats.gamma, verbose=True
):
    """
    Fit distributions to the data partitioned by thresholds.

    :param data: the data to model.
    :param thresholds: thresholds that divide data into separate distributions.
    :param zero_loc: fix location parameter for first distribution.
    :param dist: type of distribution to fit (scipy.stats).
    :param bool verbose: Print fitted distribution parameters.
    :return: list of parameters for each distribution.
    """
    param_list = []
    #     print('len thresholds',len(thresholds))
    for i in range(len(thresholds)):
        part = partition(data, thresholds, i)
        if zero_loc is not None and i == 0:
            fit = dist.fit(part, floc=zero_loc)
        else:
            fit = dist.fit(part)
        if verbose:
            print('{} distribution:{} params:{}'.format(dist.name, i, fit))
        param_list.append(list(fit) + [len(part) / len(data)])
    return param_list


@jit
def partition(data, thresholds, i):
    """
    Return the ith partition of the data as defined by thresholds.

    :param ndarray data: the data to partition.
    :param iterable thresholds: the threshold values that partition data.
    :param int i: the partition to return, the ith partition is defined as
                  threshold[i] < data <= threshold[i+1]. The last partition,
                  when i = len(thresholds-1), is defined as
    :return: ndarray of the data in the partition.
    :note: This is used to assign a hard responsibility in the expectation
           maximisation algorithm used to fit data to a mixture model.
    """

    if i == len(thresholds) - 1:
        return data[data > thresholds[i]]
    else:
        return data[
            np.bitwise_and(data > thresholds[i], data <= thresholds[i + 1])
        ]


def plot_mixture_model(
        model, data=None, x=None, normalised=False, bins=16000, figsize=None
):
    """
    Plot a mixture model optionally including a histogram of the modeled data.
    if no data is None x must not be None.

    :param MixtureModel model: the mixture model.
    :param ndarray data: the data to histogram.
    :param ndarray x: the x values to plot.
    :param bool normalised: normalise the model distributions.
    :param bins: passed to numpy.histogram().
    :param figsize: passed to matplotlib.pyplot.figure().
    :return: the figure handle.
    """

    if data is None and x is None:
        raise AttributeError('Either data or x must be supplied')

    if not isinstance(bins, int):
        bins = max(1000, bins[-1])
    else:
        bins = max(1000, bins)

    if figsize is None:
        fig = plt.figure()
    else:
        fig = plt.figure(figsize=figsize)

    ax = fig.add_axes([0.12, 0.12, 0.87, 0.87])
    if data is not None:
        hist, edges = np.histogram(data, bins=bins)
        w = edges[1] - edges[0]
        c = edges[:-1] + w / 2
        x = np.linspace(edges[0], edges[-1], bins)
        max_p = max(hist)
        ax.plot(
            c[hist.nonzero()[0]],
            hist[hist.nonzero()[0]],
            's', color='darkgray', markerfacecolor='none', mew=0.5,
            markersize=3, label='bin count'
        )
    else:
        c = x

    pdfs = np.zeros((len(model.param_list), len(c)))

    for i in range(len(model.param_list)):
        if normalised:
            pdfs[i, :] = normalised_pdf(c, model.param_list[i])
        else:
            pdfs[i, :] = scaled_pdf(c, model.param_list[i])

        if i == 0:
            ax.plot(x, pdfs[i, :] * sum(hist) * w, lw=2, label='noise')
        else:
            ax.plot(
                x, pdfs[i, :] * sum(hist) * w, lw=2,
                label='{} Photon'.format(i)
            )

    if data is None:
        max_p = max(max(pdfs))

    ax.plot(x, np.sum(pdfs, 0) * sum(hist) * w, 'k', lw=0.5, label='model')

    if not normalised:
        thresh = maximisation_step(model.param_list,False, dist=model.dist)
    else:
        thresh = model.thresholds
    for t in thresh:
        ax.plot([t, t], [0, max_p], ':k', lw=1)

    ax.set_xlabel('area')
    ax.set_ylabel('count')
    fig.legend()
    return fig

"""
timing analysis
"""


@jit
def xcor(s1, s2):
    """
    Count correlations between timestamp sequences.

    :param ndarray s1: Monotonically increasing timestamp sequence.
    :param ndarray s2: Monotonically increasing timestamp sequence.
    :return: correlation count.

    :notes: iterates over s1 and performs a linear search of  s2 for the s1
            timestamp.
    """
    i2 = 0
    c = 0
    for i in range(len(s1)):
        while i2 < len(s2) and s1[i] > s2[i2]:
            i2 += 1
        if i2 < len(s2):
            c += s1[i] == s2[i2]
    return c


@jit
def x_correlation(s1, s2, r):
    """
    Cross correlation of timestamp sequences s1 and s2 over the delay range r.

    :param ndarray s1: Monotonically increasing timestamp sequence.
    :param ndarray s2: Monotonically increasing timestamp sequence.
    :param r: range of delays added to s1
    :return: ndarray representing the cross correlation function.
    """
    x_corr = np.zeros((len(r),), np.uint64)
    for l in r:
        x_corr[l - r[0]] = xcor(s1 + l, s2)
    return x_corr


@jit
def drive_correlation(abs_time, mask, data, thresholds, r, verbose=False):
    """
    Calculate the temporal cross-correlation between a channel measuring a
    heralding signal, ie the laser drive pulse, and a channel detecting photons.
    The correlation is calculated for the different photon numbers determined by
    the thresholds parameter.


    :param ndarray abs_time: absolute timestamp sequence.
    :param ndarray mask: boolean mask that identifies the entries in abs_time
                         belonging to the photon channel, other entries are
                         assumed to be the heralding channel.
    :param ndarray data: energy measurement data for the photon channel.
    :param thresholds: thresholds that partition data into photon number.
    :param r: the range of heralding channel delays to cross correlate.
    :param bool verbose: Print progress message as each threshold is processed.
    :return: list of ndarrays representing the cross correlation.

    :note: abs_time[mask] and data must be the same length.
    """

    xc = []
    for t in range(len(thresholds)):
        if verbose:
            if t == 0:
                print('Noise partition')
            elif t == len(thresholds)-1:
                print('{}+ photon partition', t)
            else:
                print('{} photon partition', t)

        if t == len(thresholds)-1:
            xc.append(
                x_correlation(
                    abs_time[not_l(mask)],
                    abs_time[mask][data > thresholds[t]],
                    r
                )
            )
        else:
            xc.append(
                x_correlation(
                    abs_time[not_l(mask)],
                    abs_time[mask]
                    [and_b(data > thresholds[t], data < thresholds[t + 1])],
                    r
                )
            )

    return xc
